import os
import io
import re
import time
import json
import random
import unicodedata
from urllib.parse import urljoin, quote

import requests
import pandas as pd
import streamlit as st
from bs4 import BeautifulSoup

# ==============================================
# Utilidades
# ==============================================

APP_TITLE = "üè¢ Prospectador B2B ‚Äì Extra√ß√£o e Enriquecimento (v6)"

UF_NOMES = {
    "AC": "Acre", "AL": "Alagoas", "AP": "Amap√°", "AM": "Amazonas",
    "BA": "Bahia", "CE": "Cear√°", "DF": "Distrito Federal", "ES": "Esp√≠rito Santo",
    "GO": "Goi√°s", "MA": "Maranh√£o", "MT": "Mato Grosso", "MS": "Mato Grosso do Sul",
    "MG": "Minas Gerais", "PA": "Par√°", "PB": "Para√≠ba", "PR": "Paran√°",
    "PE": "Pernambuco", "PI": "Piau√≠", "RJ": "Rio de Janeiro", "RN": "Rio Grande do Norte",
    "RS": "Rio Grande do Sul", "RO": "Rond√¥nia", "RR": "Roraima", "SC": "Santa Catarina",
    "SP": "S√£o Paulo", "SE": "Sergipe", "TO": "Tocantins"
}

DDD_PA = {"91", "93", "94"}

DEFAULT_UA = (
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
    "(KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36"
)


def slug(s: str) -> str:
    s = unicodedata.normalize("NFKD", s).encode("ascii", "ignore").decode("ascii")
    return re.sub(r"[^a-z0-9]+", "-", s.lower()).strip("-")


def limpa_cnpj(cnpj: str) -> str:
    return re.sub(r"\D", "", cnpj or "")


@st.cache_data(ttl=60 * 30)
def http_get(url: str, timeout: int = 25, headers: dict | None = None) -> requests.Response | None:
    try:
        h = {"User-Agent": DEFAULT_UA}
        if headers:
            h.update(headers)
        r = requests.get(url, headers=h, timeout=timeout)
        r.raise_for_status()
        return r
    except requests.exceptions.HTTPError as e:
        # Apenas para 404, n√£o mostramos erro, s√≥ retornamos None
        if e.response.status_code != 404:
            st.warning(f"Erro de conex√£o: {e}")
        return None
    except Exception as e:
        st.warning(f"Erro inesperado de conex√£o: {e}")
        return None


# ==============================================
# Enriquecimento (baixo custo): BrasilAPI
# ==============================================

@st.cache_data(ttl=60 * 60)
def buscar_dados_receita_federal(cnpj: str) -> dict:
    c = limpa_cnpj(cnpj)
    if len(c) != 14:
        return {}

    # Usando apenas a BrasilAPI por ser mais est√°vel
    url = f"https://brasilapi.com.br/api/cnpj/v1/{c}"
    r = http_get(url, timeout=20)
    if not r:
        return {}
    try:
        data = r.json()
        if "cnpj" in data:
            return {
                "Nome": data.get("razao_social"),
                "Nome Fantasia": data.get("nome_fantasia"),
                "CNPJ": data.get("cnpj"),
                "Situa√ß√£o Cadastral": data.get("descricao_situacao_cadastral"),
                "CNAE Principal": str(data.get("cnae_fiscal")),
                "Endere√ßo": f"{data.get('logradouro', '')}, {data.get('numero', '')} - {data.get('bairro', '')}, {data.get('municipio', '')} - {data.get('uf', '')}",
                "Telefone": data.get("ddd_telefone_1"),
            }
    except Exception:
        return {}
    return {}


@st.cache_data(ttl=60 * 60)
def buscar_emails_site(website: str, timeout: int = 12) -> list[str]:
    # (Fun√ß√£o mantida como estava, sem altera√ß√µes)
    if not website or not isinstance(website, str) or not website.startswith("http"):
        return []
    r = http_get(website, timeout=timeout)
    if not r:
        return []

    emails = set()
    email_pattern = re.compile(r"\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}\b")
    emails.update(e.lower() for e in email_pattern.findall(r.text))

    try:
        soup = BeautifulSoup(r.text, "html.parser")
        for a in soup.find_all("a", href=True):
            href = a["href"].strip()
            if href.startswith("mailto:"):
                emails.add(href.replace("mailto:", "").lower())
    except Exception:
        pass

    return sorted({e for e in emails if not re.search(r"\.(png|jpg|jpeg|gif|svg)$", e)})


@st.cache_data(ttl=60 * 60)
def buscar_redes_sociais(website: str) -> dict:
    # (Fun√ß√£o mantida como estava, sem altera√ß√µes)
    redes = {"Facebook": None, "Instagram": None, "LinkedIn": None}
    if not website or not website.startswith("http"):
        return redes
    r = http_get(website, timeout=12)
    if not r:
        return redes
    try:
        soup = BeautifulSoup(r.text, "html.parser")
        for a in soup.find_all("a", href=True):
            href = a["href"].lower()
            if not redes["Facebook"] and "facebook.com" in href:
                redes["Facebook"] = a["href"]
            elif not redes["Instagram"] and "instagram.com" in href:
                redes["Instagram"] = a["href"]
            elif not redes["LinkedIn"] and "linkedin.com" in href:
                redes["LinkedIn"] = a["href"]
    except Exception:
        pass
    return redes


# ==============================================
# M√©todos de EXTRA√á√ÉO
# ==============================================

@st.cache_data(ttl=60 * 10)
def serpapi_google_maps(query: str, location: str, api_key: str, num_results: int = 50) -> list[dict]:
    # (Fun√ß√£o mantida como estava, sem altera√ß√µes)
    if not api_key:
        st.error("Por favor, insira uma chave de API da SerpAPI.")
        return []
    url = "https://serpapi.com/search"
    params = {
        "engine": "google_maps",
        "q": f"{query} {location}",
        "hl": "pt",
        "gl": "br",
        "api_key": api_key,
        "num": min(int(num_results), 100),
    }
    r = http_get(url, timeout=30)
    if not r: return []
    try:
        data = r.json()
        if "error" in data:
            st.error(f"Erro SerpAPI: {data['error']}")
            return []
        out = []
        for p in data.get("local_results", []) or []:
            out.append(
                {
                    "Nome": p.get("title"), "Endere√ßo": p.get("address"),
                    "Telefone": p.get("phone"), "Website": p.get("website"),
                    "Rating": p.get("rating"), "Avalia√ß√µes": p.get("reviews"),
                    "Origem": "SerpAPI",
                }
            )
        return out
    except Exception as e:
        st.error(f"Erro ao processar dados da SerpAPI: {e}")
        return []


# NOVA E ROBUSTA VERS√ÉO DA FUN√á√ÉO PARA A ROTA 2
@st.cache_data(ttl=60 * 30)
def buscar_cnpjs_google(termo_busca: str, max_empresas: int) -> list[dict]:
    """Busca por um termo no Google, extrai CNPJs dos resultados e enriquece com a BrasilAPI."""
    if not termo_busca:
        return []

    st.info(f"Buscando no Google por: '{termo_busca}'...")
    url_encoded_term = quote(termo_busca)
    url = f"https://www.google.com/search?q={url_encoded_term}&num=20"
    
    r = http_get(url)
    if not r:
        st.error("Falha ao conectar com o Google. O Google pode ter bloqueado a requisi√ß√£o.")
        return []

    soup = BeautifulSoup(r.text, "html.parser")
    texto_pagina = soup.get_text()
    
    # Padr√£o de regex para encontrar CNPJs no texto da p√°gina
    cnpj_pattern = re.compile(r'\d{2}\.\d{3}\.\d{3}/\d{4}-\d{2}')
    cnpjs_encontrados = set(cnpj_pattern.findall(texto_pagina))

    if not cnpjs_encontrados:
        st.warning("Nenhum CNPJ encontrado nos resultados da busca do Google.")
        return []

    st.info(f"{len(cnpjs_encontrados)} CNPJs √∫nicos encontrados. Buscando dados...")
    
    resultados = []
    for cnpj in list(cnpjs_encontrados)[:max_empresas]:
        dados_empresa = buscar_dados_receita_federal(cnpj)
        if dados_empresa:
            dados_empresa["Origem"] = "Google + BrasilAPI"
            resultados.append(dados_empresa)
        time.sleep(0.5) # Pausa para n√£o sobrecarregar a API

    return resultados


# ==============================================
# Orquestra√ß√£o de ENRIQUECIMENTO (Simplificada)
# ==============================================

def enriquecer_empresas(empresas: list[dict], *, buscar_redes: bool, buscar_emails: bool) -> list[dict]:
    # (Fun√ß√£o mantida, por√©m o enriquecimento principal j√° acontece na busca da Rota 2)
    # ...
    # (O restante da fun√ß√£o continua igual)
    out = []
    total = len(empresas)
    if total == 0: return []
    
    pb = st.progress(0.0, text="Iniciando enriquecimento...")
    msg = st.empty()

    for i, emp in enumerate(empresas, start=1):
        msg.write(f"Enriquecendo: {emp.get('Nome') or emp.get('CNPJ') or 'registro'} ({i}/{total})")
        row = dict(emp)

        if (buscar_emails or buscar_redes) and not row.get("Website"):
            # Tenta buscar o site se n√£o houver
            try:
                from googlesearch import search
                query = f"{row.get('Nome')} {row.get('Endere√ßo')}"
                for j in search(query, num=1, stop=1, pause=2):
                    row["Website"] = j
                    break
            except Exception:
                pass # Ignora se a busca falhar

        if buscar_emails and row.get("Website"):
            emails = buscar_emails_site(row["Website"]) or []
            row["Emails do Site"] = ", ".join(emails) if emails else None

        if buscar_redes and row.get("Website"):
            row.update(buscar_redes_sociais(row["Website"]))

        out.append(row)
        pb.progress(i / total, text=f"Processando {i}/{total}...")
        time.sleep(random.uniform(0.1, 0.25))

    pb.empty(); msg.empty()
    return out


# ==============================================
# Interface Streamlit
# ==============================================

def main():
    st.set_page_config(page_title=APP_TITLE, page_icon="üè¢", layout="wide")
    st.title(APP_TITLE)

    tab_busca, tab_refino, tab_result = st.tabs(["üîé Busca", "üßπ Filtro", "üìä Resultados"])

    with tab_busca:
        metodo = st.selectbox(
            "M√©todo de aquisi√ß√£o",
            (
                "Rota 1 ‚Äì SerpAPI Google Maps (Alta Performance)",
                "Rota 2 ‚Äì Busca Google + CNPJ (Gratuita, Melhor Esfor√ßo)",
                "Rota 3 ‚Äì Importar CSV",
            ),
        )

        col = st.columns(3)
        termo_busca = "prospects"
        
        if metodo.startswith("Rota 1"):
            with col[0]:
                nicho = st.text_input("üéØ Nicho / Palavra-chave", value="mineradora")
                termo_busca = nicho
            with col[1]:
                local = st.text_input("üìç Localiza√ß√£o", value="Par√°, Brasil")
            with col[2]:
                serp_key = st.text_input("üîë SerpAPI Key", type="password")
            max_r = st.slider("Qtd m√°x. resultados", 10, 100, 40, 10)

        elif metodo.startswith("Rota 2"):
            with col[0]:
                termo = st.text_input("üîé Termo de Busca", value="empresas de extra√ß√£o de min√©rio de ferro em Minas Gerais")
                termo_busca = termo
            with col[1]:
                max_r = st.slider("Qtd m√°x. de empresas a buscar", 5, 50, 15, 5)
            with col[2]:
                st.write(" ") # Espa√ßamento
                st.info("Esta rota busca CNPJs no Google e valida os dados. Pode ser inst√°vel.")

        else: # Rota 3
            up = st.file_uploader("Envie um CSV com CNPJ, Nome, Website", type=["csv"])
            df_import = None
            if up is not None:
                try: df_import = pd.read_csv(up)
                except Exception:
                    try: df_import = pd.read_csv(up, sep=";")
                    except Exception: st.error("N√£o foi poss√≠vel ler o CSV.")
            max_r = st.slider("Qtd m√°x. linhas", 10, 2000, 200, 10)
            termo_busca = "importado"

        st.markdown("---")
        # Para Rota 2, o enriquecimento principal j√° foi feito. Estas op√ß√µes s√£o para dados adicionais.
        st.write("**Op√ß√µes de Enriquecimento Adicional:**")
        use_emails = st.checkbox("Buscar e-mails no site (Processo Lento)", value=False)
        use_social = st.checkbox("Localizar redes sociais no site (Processo Lento)", value=False)

        if st.button("üöÄ Executar Busca", type="primary"):
            registros: list[dict] = []
            with st.spinner("Buscando‚Ä¶"):
                if metodo.startswith("Rota 1"):
                    registros = serpapi_google_maps(nicho, local, serp_key, num_results=max_r)
                elif metodo.startswith("Rota 2"):
                    registros = buscar_cnpjs_google(termo, max_empresas=max_r)
                else: # Rota 3
                    if df_import is not None and len(df_import) > 0:
                        tmp = df_import.head(max_r).to_dict(orient="records")
                        for r in tmp:
                            r.setdefault("Nome", r.get("nome") or r.get("Razao Social"))
                            r.setdefault("CNPJ", r.get("cnpj") or r.get("CNPJ (Receita)"))
                            r.setdefault("Website", r.get("website") or r.get("site"))
                            r.setdefault("Origem", "CSV")
                        registros = tmp
                    else:
                        st.warning("Envie um CSV v√°lido.")

            if registros:
                st.session_state["termo_busca"] = termo_busca
                if use_emails or use_social:
                     with st.spinner("Enriquecendo com dados de sites... (pode demorar)"):
                        registros = enriquecer_empresas(
                            registros,
                            buscar_redes=use_social,
                            buscar_emails=use_emails,
                        )
                st.session_state["resultados"] = registros
                st.success(f"{len(registros)} registros encontrados!")
            else:
                st.warning("Nenhum registro encontrado para os crit√©rios fornecidos.")
                st.session_state["resultados"] = []

    # O restante das abas (Refino e Resultados) continua funcional como antes
    with tab_refino:
        df = pd.DataFrame(st.session_state.get("resultados", []))
        if df.empty:
            st.info("Sem dados ainda. Execute uma busca na primeira aba.")
        else:
            # ... (c√≥digo de filtro mantido)
            colf = st.columns(3)
            with colf[0]:
                filtro_uf = st.selectbox("Filtrar por UF", ["(sem filtro)"] + list(UF_NOMES.keys()))
            with colf[1]:
                filtra_cnae_ini = st.text_input("CNAE Principal come√ßa com (ex.: 07, 46)")
            with colf[2]:
                somente_ddd_pa = st.checkbox("Apenas DDD do Par√° (91/93/94)", value=False)

            df2 = df.copy()
            if filtro_uf != "(sem filtro)":
                df2 = df2[df2["Endere√ßo"].fillna("").str.contains(filtro_uf, case=False, regex=False)]
            if filtra_cnae_ini:
                df2["CNAE Principal"] = df2["CNAE Principal"].astype(str)
                df2 = df2[df2["CNAE Principal"].fillna("").str.startswith(filtra_cnae_ini)]
            if somente_ddd_pa:
                df2 = df2[df2["Telefone"].fillna("").str.replace(r'\D', '', regex=True).str[:2].isin(DDD_PA)]

            st.write(f"Resultados filtrados: {len(df2)}")
            st.dataframe(df2)
            st.session_state["df_filtrado"] = df2

    with tab_result:
        st.header("üì• Download dos Resultados")
        df_final = st.session_state.get("df_filtrado", pd.DataFrame(st.session_state.get("resultados", [])))
        if df_final.empty:
            st.warning("Nenhum resultado para exibir ou baixar.")
        else:
            # ... (c√≥digo de download mantido)
            st.dataframe(df_final)
            output = io.BytesIO()
            with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
                df_final.to_excel(writer, index=False, sheet_name='Prospects')
            excel_data = output.getvalue()
            file_name_slug = slug(st.session_state.get("termo_busca", "prospects"))
            st.download_button(
                label="üì• Baixar resultados em Excel (.xlsx)",
                data=excel_data,
                file_name=f"prospects_{file_name_slug}.xlsx",
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
            )

if __name__ == "__main__":
    main()
